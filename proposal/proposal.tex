\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[nonatbib]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{TODO}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Johann Lingohr \\
  \texttt{johannlingohr\@gmail.com} \\
  \And
  Jiefei Li \\
  \texttt{jeffuvic@ece.ubc.ca} \\
  \And
  Jay Fu \\
  \texttt{ngaifu16\@gmail.com} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Introduction}

Generating realistic images is an important area in computer vision because it provides insights into how our algorithms understand the visual world and has practical use for artists and graphic designers. With developments in machine learning and deep learning algorithms, there has been a significant focus on new image generating algorithms. Generative adversarial networks (GANs) \cite{gan} have become one of the most popular neural network models used to generate realistic images. These models work by jointly training an image generator for image synethesis and a discriminator for determining whether the synthesized image is fake or real. Extending this model, \textit{Reed et al.} \cite{t2im} generate images using GANs conditioning on text and are able to generate high-resolution images. \textit{Huang et al.} \cite{stackedgan} further improve on this using a two-stage architecture resulting in 256x256 photo-realistic images. This approach shows us, importantly, that we can break up hard tasks into manageable sub-tasks to improve results, going from a coarse-to-fine level of detail in the process. \textit{Xu et el.} \cite{attengan} go further by leveraging the attention mechanism to draw images region-by-region conditioned on relevant words in a long text description, resulting in images with much more detail. Finally, Nvidia \cite{stylegan} incorporate style transfer to create a style-based image generator to replace the original generator in the GAN model, bringing the realism of synthesized images to an impressive level.

While these techniques generate realistic looking images when the input sentence describes simple scenes, they do not perform well when trying to generate images corresponding to more complex scenes. Branching from these GAN models is another approach to discover the role of scene graph and scene layout in generating images from text. These allows us to explicitly represent objects and relationships in complex sentences. \textit{Johnson et al.} \cite{sg2im} use graph convolution neural networks to extract features and objects from scene graphs and are able to generate images preserving relationships among multiple objects, but the images are low-resolution only and does is unable to generate good images when there are multiple relationships among the objects. \textit{Tripathi et al.} \cite{sg2imgcontext} attempt to improve on this by introducing scene graph context to encourage generated images to appear realistic and better respect the scene graph relationships.

The goal of this project is to build on these methods. Specifically, we aim to improve the realism of synethesized images.

\section{Method}

Our framework builds from \cite{sg2im} and \cite{sg2imgcontext}. In their method a scene graph is encoded into object embedding vectors using a Graph Convolution Neural Network. These vectors are transformed into a scene layout using an object generator network consisting of a mask regression network to predict segmentation masks and a bounding box regression network to predict bounding boxes. These are combined to form the object layout and summing over all object layouts producing the scene layout. The scene layout and random noise is then passed to a Cascade Refinement Network (CRN) \cite{crn} consisting of several upsampling modules of increasing size to generate the final image. In particular, we implement a modified CRN in which skip layer connections are added to improve upsampling quality. We also incorporate a scene graph context \cite{sg2imgcontext} that pools features from the graph convolution neural network. The features are then embedded by a fully connected network and fed to each upsampling module in CRN to provide context to the image generator.

Building on these methods, instead of feeding the embedding to the first convolution layer of the CRN, we will pass the embedding to each upsampling layer in the CRN so that more relational context could be preserved during the upsample process. Another improvement is the skip layer connection in between the CRN upsampling layers. This helps the model to better preserve original features in the forward path.

% \section{Related Work}


% Another important branch of GAN models is to discover the role of scene graph and scene layout in synthesizing images from text. Johnson et al.\cite{sg2im} proposed a image synthesis neural network not only combining the GAN with a recurrent neural network and also innovatively used a graph convolution neural network to extract features and objects from Scene Graph, the model in this paper can generate images which perserve relationship among multiple objects but are low-resolution only. Tripathi et al. \cite{sg2imgcontext} further improved the achievements in \cite{sg2im} by doing the following things, firstly features are pooled from the scene graph convolution network as scene context, then the scene context are fed into both image generation network and the image discriminator, as a result, not only the realism of the images was enhanced and also the object relationships are better preserved. These two papers greatly inspired our work.

% There are also papers focused on how to effectively generating scene graph from text descriptions. Schuster et al. \cite{scenegraph} proposed a model to automatically generating scene graph from natural language scene description and also a rule-based parser and a classifier-based parser which can be used for image synthesis. On the other side, Hongy et al. brough out a hierachical approach for generating semantic layout directly from text, the layout generator decomposes the generation process into multiple stages in a coarse-to-fine manner. This approach allows us to skip the step to generate scene graph and achieve a semantic layout for image generation directly.

% Besides the mainstream method using GANs, there are also approaches using cascaded refinement network (CRN) which is worth serious attention. Chen et al. \cite{crn} presented an approach to synthesizing images conditioned on scene layout using CRN, compared with GAN models, CRN has the advantages such as simpler structure, faster training speed and plausible high-resolution outcome image.

% Our mission is to evaluate some of the work mentioned above and discover a novel way to improve the realism of synthesized images based on predecessor's reseach achievements.


\section{Experiments}

\subsection{Datasets}

Similar to previous methods, we will evaluate our model using the COCO-Stuff [\cite{cocostuff}] and Visual Genome [\cite{visualgen}] datasets. The COCO-Stuff dataset contains 40k training and 5k validation annotated images with bounding boxes and segmentation masks, as well as providing 80 thing categories and 90 stuff categories. Similar to \cite{sg2im} and \cite{sg2imgcontext} we can use the image captions and annotation to construct scene graphs based on the 2D image coordinates of the objects. The Visual Genome dataset contains over 100k images annotated with their scene graphs.

\subsection{Evaluation}

In order to evaluate a generated image we need a measure that answers 1) how realistic does the generated image appear? 2) how well can we recognize the different objects in the image? 3) How diverse are the generated images?

To this end we use the Inception Score \cite{inception}, which evaluates both the quality of generated images and diversity by applying a pre-trained classifier on generated images. Correctly predicting class labels for objects in the generated image should correspond to the generated images looking realistic.

In order to demonstrate the model's ability to generate diverse output images, we will be interpolating between various input object vectors to see how the result smoothly transform from one to the other.


\begin{thebibliography}{1}

\medskip
\small


\bibitem{stackedgan}
Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft and Serge Belongie.
\newblock Stacked Generative Adversarial Networks, 2016;
\newblock arXiv:1612.04357.

\bibitem{t2im}
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele and Honglak Lee.
\newblock Generative Adversarial Text to Image Synthesis, 2016;
\newblock arXiv:1605.05396.

\bibitem{sg2im}
Justin Johnson, Agrim Gupta and Li Fei-Fei.
\newblock Image Generation from Scene Graphs, 2018;
\newblock arXiv:1804.01622.

\bibitem{sg2imgcontext}
Subarna Tripathi, Anahita Bhiwandiwalla, Alexei Bastidas and Hanlin Tang.
\newblock Using Scene Graph Context to Improve Image Generation, 2019;
\newblock arXiv:1901.03762.

\bibitem{cocostuff}
Holger Caesar, Jasper Uijlings and Vittorio Ferrari.
\newblock COCO-Stuff: Thing and Stuff Classes in Context, 2016;
\newblock arXiv:1612.03716.

\bibitem{visualgen}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein and Fei-Fei Li.
\newblock Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations, 2016;
\newblock arXiv:1602.07332.

\bibitem{inception}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford and Xi Chen.
\newblock Improved Techniques for Training GANs, 2016;
\newblock arXiv:1606.03498.


\bibitem{crn}
Qifeng Chen and Vladlen Koltun.
\newblock Photographic Image Synthesis with Cascaded Refinement Networks, 2017;
\newblock arXiv:1707.09405.

\bibitem{gan}
Ian J. Goodfellow,
Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,Aaron Courville and Yoshua Bengio.
\newblock Generative Adversarial Nets, 2014;
\newblock arXiv:1406.2661.

\bibitem{attengan}
Tao Xu, Pengchuan Zhang, Qiuyuan Huang,
Han Zhang, Zhe Gan, Xiaolei Huang and Xiaodong He.
\newblock AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks;
\newblock arXiv:1711.10485.

\bibitem{stylegan}
Tero Karras, Samuli Laine and Timo Aila.
\newblock A Style-Based Generator Architecture for Generative Adversarial Networks;
\newblock arXiv:1812.04948.

\bibitem{scenegraph}
Sebastian Schuster, Ranjay Krishna, Angel Chang,
Li Fei-Fei, and Christopher D. Manning.
\newblock Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval;

\bibitem{scenelayout}
Seunghoon Hongy, Dingdong Yangy, Jongwook Choiy, Honglak Lee.
\newblock Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis;
\newblock arXiv:1801.05091.


\end{thebibliography}

% \medskip
% \small
% \bibliography{bib}

\end{document}