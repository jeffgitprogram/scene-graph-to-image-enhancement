\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{TODO}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Johann Lingohr \\
  \texttt{johannlingohr\@gmail.com} \\
  \And
  Author \\
  \texttt{email} \\
  \And
  Author \\
  \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

Generating realistic images is an important area in computer visision because it provides insights into how our algorithmns understand the visual world and has practical use for artists and graphic designers. With developments in machine learning and deep learning algorithmns, there has been a significant focus on new image generating algorithmns. \citet{t2im} and \citet{stackedgan} generate images using GANs conditioning on text. While these techniques generate realistic looking images when the input sentence describes simple scenes, it does not perform well when trying to generate images corresponding to more complex scenes. \citet{sg2im} instead generate images from scene graphs, which can explicitly represent objects and relationships in complex sentences. This method generates images that better preserve the structure in the given input, but still appears to struggle when there are a lot of objects with multiple relationships. \citet{sg2imgcontext} attempt to improve on this by introducing scene graph context to encourage generated images to appear realistic and better respect the scene graph relationships.

Building on these methods, (What are we going to do)

\section{Related Work}

(Maybe just part of intro for proposal)

\section{Method}

\section{Experiments}

\subsection{Datasets}

Similar to previous methods, we will evaluate our model using the COCO-Stuff [\citet{cocostuff}] and Visual Genome [\citet{visualgen}] datasets. The COCO-Stuff dataset contains 40k training and 5k validation annotated images with bounding boxes and segmentation masks, as well as providing 80 thing categories and 90 stuff categories. Similar to \citet{sg2im} and \citet{sg2imgcontext} we can use the image captions and annotation to construct scene graphs based on the 2D image cootdinates of the objects. The Visual Genome dataset contains over 100k images annotated with their scene graphs.

\subsection{Evaluation}

In order to evaluate a generated image we need a measure that answers 1) how realistic does the generated image appear? 2) how well can we recognize the different objects in the image? 3) How diverse are the generated images?

To this end we use to Inception Score, which evaluates both the quality of generated images and diversity.


\medskip
\small
\bibliography{bib}

\end{document}